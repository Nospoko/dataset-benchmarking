{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import fortepyan as ff\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from data_midi_bert import augment as A\n",
    "from data_midi_bert.masking import Mask, AwesomeMasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "\n",
    "\n",
    "def convert_dataset(\n",
    "    dataset: Dataset,\n",
    "    masks: list[Mask],\n",
    "    dataset_cfg: DictConfig,\n",
    ") -> list[dict]:\n",
    "    # TODO This needs multiprocessing, consider dataset.map(convert, num_proc=32)\n",
    "    records = []\n",
    "    for record in tqdm(dataset):\n",
    "        piece = ff.MidiPiece.from_huggingface(record)\n",
    "\n",
    "        if piece.size < dataset_cfg.sequence_len:\n",
    "            continue\n",
    "\n",
    "        # This is an alternative way of calculating n_samples\n",
    "        # Idea was: instead of constant step moving window, let's have\n",
    "        # random sampling of fragment start\n",
    "        records += process_piece(\n",
    "            piece=piece,\n",
    "            masks=masks,\n",
    "            dataset_cfg=dataset_cfg,\n",
    "        )\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def process_piece(\n",
    "    piece: ff.MidiPiece,\n",
    "    masks: list[Mask],\n",
    "    dataset_cfg: DictConfig,\n",
    ") -> list[dict]:\n",
    "    # How many samples we should produce from this piece\n",
    "    n_samples = dataset_cfg.n_augments * (piece.size // dataset_cfg.sequence_step)\n",
    "\n",
    "    # Every part of the piece we want to sample is defined by\n",
    "    # first note index - here we select N of those randomly\n",
    "    # +1 because random returns [low, high)\n",
    "    high = piece.size + 1 - dataset_cfg.sequence_len\n",
    "    starts = np.random.randint(0, high, size=n_samples)\n",
    "\n",
    "    # Inject the dstart feature to the whole piece\n",
    "    piece.df[\"next_start\"] = piece.df.start.shift(-1)\n",
    "    piece.df[\"dstart\"] = piece.df.next_start - piece.df.start\n",
    "    piece.df.dstart = piece.df.dstart.fillna(0)\n",
    "\n",
    "    records = []\n",
    "    for start in starts:\n",
    "        # Required so it's json serializable\n",
    "        start = int(start)\n",
    "        finish = start + dataset_cfg.sequence_len\n",
    "\n",
    "        part = piece[start:finish]\n",
    "\n",
    "        # Is this a piano piece?\n",
    "        if part.df.pitch.min() < 21 or part.df.pitch.max() > 108:\n",
    "            continue\n",
    "\n",
    "        # Random augments\n",
    "        part.df, speedup_factor = A.change_speed(part.df)\n",
    "        part.df, pitch_shift = A.pitch_shift(part.df)\n",
    "        part.source |= {\"pitch_shift\": pitch_shift, \"speedup_factor\": speedup_factor}\n",
    "\n",
    "        record = {\n",
    "            \"pitch\": part.df.pitch.astype(\"int16\").values.T,\n",
    "            \"start\": part.df.start.values,\n",
    "            \"dstart\": part.df.dstart.values,\n",
    "            \"end\": part.df.end.values,\n",
    "            \"duration\": part.df.duration.values,\n",
    "            \"velocity\": part.df.velocity.values,\n",
    "            \"source\": json.dumps(part.source),\n",
    "        }\n",
    "        # Masking\n",
    "        masking_spaces = {}\n",
    "        for mask in masks:\n",
    "            masking_space = mask.masking_space(part.df).values\n",
    "            masking_spaces[mask.token] = masking_space\n",
    "\n",
    "        record[\"masking_space\"] = masking_spaces\n",
    "        records.append(record)\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/samue/.cache/huggingface/datasets/roszcz___parquet/roszcz--maestro-v1-sustain-5350ada51983a2ef/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65886a91f4dd4bfb94b18fedc994d6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roszcz/maestro-v1-sustain\")\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cfg = DictConfig(\n",
    "        {\n",
    "            \"sequence_len\": 60,\n",
    "            \"sequence_step\": 60,\n",
    "            \"n_augments\": 5,\n",
    "        }\n",
    "    )\n",
    "midi_masks = AwesomeMasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 27/177 [00:23<02:05,  1.19it/s]"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 1\n",
    "\n",
    "records = convert_dataset(dataset=test_dataset,\n",
    "            dataset_cfg=dataset_cfg,\n",
    "            masks=midi_masks.masks,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
